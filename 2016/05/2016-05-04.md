## DB

### ["Turning the database inside out with Apache Samza" by Martin Kleppmann](https://www.youtube.com/watch?v=fU9hR3kiOK0)

[Transcript](http://www.confluent.io/blog/turning-the-database-inside-out-with-apache-samza/)

* Databases have been **global, shared, mutable states**. NoSQL didn't make any difference to it.
* Think of a database as an **always-growing collection of immutable facts**.
* Derived data
  * Replication
    * Your master node might send some writer-logs/logic-logs to the slaves to inform them about the updates. That log contains immutable facts(about mutation).
  * Secondary index
    * Maintained by DB
    * Create indexes concurrently(since for some big tables it could take hours to build a secondary index)
  * Caching
    * How to invalidate it?
    * Race condition/concurrency issues
    * Cold start
  * Materialized views
    * Maintained by DB
    * It's actually kinda like caching
* A more fruitful approach is to **take the streams of facts** as they come in, and **functionally process them** in real-time - unbundled DB e.g. Kafka
  * Event stream/transaction logs, append the write to the stream
  * We can derive our materialized views from these logs
  * It's just stream processing - that's where Apache Samza comes in
  * Kappa architecture(in contrast to the lambda architecture)
  * Stream of events
* Apache Samza's core is a distributed, durable commit log, implemented by Apache Kafka. Layered on top are simple but powerful tools for joining streams and managing large amounts of data reliably

#### Why

* Better data
  * Normalization v.s. denormalization - optimized for reading or writing?
  * With the new model you can separate them into different views and optimize(normalize/denormalize) them differently
  * You can build those materialized views functionally, which is very clean and simple
* Your materialized views are actually fully precomputed cache
  * There is no such thing as cold start
  * No cache miss either
  * No need to worry about invalidation(the only way to change the materialized views is to append writes to the log, it doesn't affect the business logic)
  * No race condition, the streams will be processed sequentially(you only parallelize different processes)
* Streams everywhere
  * e.g. you can think of the HTML DOM as a materialized view of the JSON you obtain from your Web API
  * The browsers are good at this reactive thingy, the frontend is starting to improve(react and FRP), but the backend is still...meh
  * Let the clients subscribe to MV changes(currently the weakest link in the pipeline is the database)
  * From request/response to subscribe/notify(e.g. meteor)

#### Q&A

* Dealing with Kafka's at-least-once delivery
  * Updating MV is idempotent most of the times
  * Tracking transaction IDs
* Dealing with asynchronous DB(reading right after writing is not necessarily getting the results of writing)
  * Transaction protocols on top of the stream model
* Dealing with schema changes over time
  * Serialization formats and versions of formats
* Security and permissions
  * Happens at the point when the data is pushed out to the subscribers/whether the subscription can take place
* Compaction
  * Keyspace compaction
* In-process key-value store in the machine
  * Currently using levelDB, considering switching to RocksDB
